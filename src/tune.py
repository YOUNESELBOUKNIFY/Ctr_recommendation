import optuna
import torch
import yaml
import os
import sys
import numpy as np

# Ajout du chemin src
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from dataloader import MMCTRDataLoader
from model import build_model
from utils import set_seed, compute_auc

# ========================
# 1. Configuration Globale
# ========================
config_path = "../config/tabtransformer_config.yaml"
if not os.path.exists(config_path):
    config_path = "config/tabtransformer_config.yaml"

with open(config_path, "r") as f:
    raw_cfg = yaml.safe_load(f)

dataset_id = raw_cfg["dataset_id"]
dataset_cfg = raw_cfg["dataset_config"][dataset_id]
base_exp_id = raw_cfg.get("base_expid", "TabTransformer_default")
base_model_cfg = raw_cfg[base_exp_id]

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Tuning on device: {device}")

# V√©rification Multi-GPU au d√©marrage
n_gpus = torch.cuda.device_count()
if n_gpus > 1:
    print(f"üöÄ Multi-GPU activ√© : {n_gpus} GPUs seront utilis√©s pour chaque essai.")

# ========================
# 2. Fonction Objective (Le c≈ìur d'Optuna)
# ========================
def objective(trial):
    """
    Cette fonction est ex√©cut√©e √† chaque essai (trial).
    Elle doit retourner la m√©trique √† maximiser (ici l'AUC).
    """
    
    # --- A. D√©finition de l'espace de recherche (Hyperparam√®tres) ---
    # Optuna va choisir des valeurs ici
    
    # 1. Param√®tres d'apprentissage
    lr = trial.suggest_float("learning_rate", 1e-4, 1e-2, log=True)
    batch_size = trial.suggest_categorical("batch_size", [4096, 8192, 16384])
    
    # 2. Param√®tres du mod√®le (Architecture)
    emb_dim = trial.suggest_categorical("embedding_dim", [32, 64, 128])
    n_layers = trial.suggest_int("transformer_n_layers", 1, 4)
    n_heads = trial.suggest_categorical("transformer_n_heads", [2, 4, 8])
    dropout = trial.suggest_float("dropout", 0.0, 0.5)
    
    # Mise √† jour de la config pour cet essai
    current_cfg = base_model_cfg.copy()
    current_cfg["learning_rate"] = lr
    current_cfg["batch_size"] = batch_size
    current_cfg["embedding_dim"] = emb_dim
    current_cfg["transformer_n_layers"] = n_layers
    current_cfg["transformer_n_heads"] = n_heads
    current_cfg["transformer_dropout"] = dropout
    current_cfg["net_dropout"] = dropout
    
    # --- B. Pr√©paration des Donn√©es et Mod√®le ---
    # On fixe une seed diff√©rente par trial ou la m√™me pour comparer purement les hyperparams
    set_seed(2025) 
    
    train_loader = MMCTRDataLoader(
        feature_map=None,
        data_path=dataset_cfg["train_data"],
        item_info_path=dataset_cfg["item_info"],
        batch_size=batch_size,
        shuffle=True,
        num_workers=4,
        max_len=int(current_cfg.get("max_len", 5))
    )

    valid_loader = MMCTRDataLoader(
        feature_map=None,
        data_path=dataset_cfg["valid_data"],
        item_info_path=dataset_cfg["item_info"],
        batch_size=batch_size,
        shuffle=False,
        num_workers=4,
        max_len=int(current_cfg.get("max_len", 5))
    )
    
    model = build_model(None, current_cfg)
    
    # --- ACTIVATION MULTI-GPU ---
    if torch.cuda.device_count() > 1:
        model = torch.nn.DataParallel(model)
        
    model.to(device)
    
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    loss_fn = torch.nn.BCELoss()

    # --- C. Boucle d'entra√Ænement All√©g√©e (Max 5-10 √©poques pour tester vite) ---
    epochs = 5  # Pas besoin de 100 epochs pour savoir si les hyperparams sont bons
    
    best_valid_auc = 0.0

    for epoch in range(epochs):
        model.train()
        for batch_dict, labels in train_loader:
            for k, v in batch_dict.items():
                batch_dict[k] = v.to(device)
            labels = labels.to(device)

            optimizer.zero_grad()
            y_pred = model(batch_dict)
            loss = loss_fn(y_pred, labels)
            loss.backward()
            optimizer.step()

        # --- D. Validation ---
        model.eval()
        y_trues, y_preds = [], []
        with torch.no_grad():
            for batch_dict, labels in valid_loader:
                for k, v in batch_dict.items():
                    batch_dict[k] = v.to(device)
                
                y_pred = model(batch_dict)
                y_trues.append(labels.cpu().numpy())
                y_preds.append(y_pred.cpu().numpy())

        if len(y_trues) > 0:
            auc = compute_auc(np.concatenate(y_trues), np.concatenate(y_preds))
        else:
            auc = 0.0
            
        # === AFFICHAGE DE L'AUC EN TEMPS R√âEL ===
        print(f"Trial {trial.number} | Epoch {epoch+1} | Valid AUC: {auc:.4f}")
        
        # --- E. Pruning (Arr√™t automatique si mauvais r√©sultats) ---
        # Optuna compare l'AUC actuel aux meilleurs essais pr√©c√©dents.
        # S'il est trop bas, on arr√™te tout de suite (TrialPruned).
        trial.report(auc, epoch)
        if trial.should_prune():
            # Petit message pour dire qu'on arr√™te
            print(f"Trial {trial.number} pruned at epoch {epoch+1}.")
            raise optuna.exceptions.TrialPruned()
            
        best_valid_auc = max(best_valid_auc, auc)

    # Nettoyage GPU apr√®s chaque essai pour √©viter "Out of Memory"
    del model
    del optimizer
    del train_loader
    del valid_loader
    torch.cuda.empty_cache()

    return best_valid_auc

# ========================
# 3. Lancement de l'√©tude
# ========================
if __name__ == "__main__":
    # On cr√©e une √©tude qui cherche √† MAXIMISER l'AUC
    study = optuna.create_study(
        direction="maximize", 
        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=1)
    )
    
    print("D√©but de l'optimisation des hyperparam√®tres...")
    # n_trials = nombre d'essais diff√©rents (ex: 20 configurations diff√©rentes)
    study.optimize(objective, n_trials=20)

    print("\n==================================")
    print("Optimisation termin√©e !")
    print("Meilleurs hyperparam√®tres trouv√©s :")
    print(study.best_params)
    print(f"Meilleur AUC : {study.best_value}")
    print("==================================")
    
    # Sauvegarder les meilleurs params dans un fichier YAML pour les r√©utiliser
    with open("best_hyperparams.yaml", "w") as f:
        yaml.dump(study.best_params, f)
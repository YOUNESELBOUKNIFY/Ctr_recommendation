Solution MMCTR Track 2 : Architecture xDeepFM Enhanced

Ce d√©p√¥t contient la solution bas√©e sur le mod√®le xDeepFM (eXtreme Deep Factorization Machine), am√©lior√© pour le challenge Multimodal.
Ce mod√®le combine la puissance des interactions explicites (CIN) avec un m√©canisme d'Attention pour l'historique utilisateur.

üìå Architecture du Mod√®le

Le mod√®le prend en entr√©e des donn√©es h√©t√©rog√®nes (Sparse IDs, Dense Embeddings, S√©quences) et produit une probabilit√© de clic (CTR).

Sch√©ma des Entr√©es/Sorties

graph TD
    %% -- INPUTS --
    subgraph "1. Entr√©es (Inputs)"
        UserID[User Placeholder]
        Context[Contexte<br/>(Likes/Views)]
        ItemID[Item ID]
        ItemImg[Item Image<br/>(128d Float)]
        History[Historique S√©quentiel<br/>(Liste d'IDs)]
    end

    %% -- EMBEDDING & PROJECTION --
    subgraph "2. Embedding & Projection Layer"
        Emb_User[User Emb]
        Emb_Ctx[Context Emb]
        Emb_Item[Item Emb]
        
        Proj_Img[<b>Projection Multimodale</b><br/>Linear + LayerNorm + DICE]
        
        Attn_Hist[<b>Attention Pooling</b><br/>(Target-Aware)]
    end

    %% CONNEXIONS COUCHE 2
    UserID --> Emb_User
    Context --> Emb_Ctx
    ItemID --> Emb_Item
    ItemImg --> Proj_Img
    
    %% Target pour Attention
    Emb_Item -.-> TargetComb
    Proj_Img -.-> TargetComb
    TargetComb[Target: ID + Image] -.-> Attn_Hist
    History --> Attn_Hist

    %% -- FEATURE STACK --
    Stack[<b>Stacked Features</b><br/>(Batch, 6 Champs, 128d)]
    Emb_User --> Stack
    Emb_Ctx --> Stack
    Emb_Item --> Stack
    Proj_Img --> Stack
    Attn_Hist --> Stack

    %% -- xDeepFM CORE --
    subgraph "3. xDeepFM Core (3 Branches)"
        direction TB
        
        %% Branche 1 : Linear
        Linear[<b>Linear Component</b><br/>(1st Order)<br/>Capture les biais globaux]
        
        %% Branche 2 : CIN
        CIN[<b>CIN (Compressed Interaction Network)</b><br/>(Explicit High-Order)<br/>Capture les interactions vectorielles]
        
        %% Branche 3 : DNN
        DNN[<b>DNN (Deep Neural Network)</b><br/>(Implicit High-Order)<br/>Capture les relations non-lin√©aires]
        
        Stack --> Linear
        Stack --> CIN
        Stack --> DNN
    end

    %% -- OUTPUT --
    subgraph "4. Sortie"
        Sum((Somme))
        Sigmoid{Sigmoid}
        Output[<b>Score CTR</b><br/>Probabilit√© [0-1]]
        
        Linear --> Sum
        CIN --> Sum
        DNN --> Sum
        Sum --> Sigmoid --> Output
    end

    style ItemImg fill:#ffe0b2,stroke:#e65100,stroke-width:2px
    style Proj_Img fill:#fff3e0,stroke:#e65100
    style CIN fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    style DNN fill:#e8f5e9,stroke:#1b5e20,stroke-width:2px
    style Attn_Hist fill:#f3e5f5,stroke:#4a148c,stroke-width:2px


üß† Composants Cl√©s

1. Entr√©es Multimodales (Image)

Le vecteur image (128d) ne passe pas par une table d'embedding classique. Il traverse un module de projection :
Linear(128->128) ‚ûî LayerNorm ‚ûî DICE Activation.
Cela permet d'aligner l'espace s√©mantique de l'image avec celui des IDs.

2. Attention Pooling (Am√©lioration)

Contrairement au xDeepFM standard qui fait une moyenne de l'historique, nous utilisons un m√©canisme d'attention (inspir√© de DIN).

Query : L'item cible (ID + Image).

Key/Value : Les items de l'historique.

R√©sultat : L'historique est pond√©r√© dynamiquement selon la pertinence avec la cible.

3. Les 3 Branches de Pr√©diction

Linear : M√©morisation simple des caract√©ristiques ("Wide").

CIN (Compressed Interaction Network) : Interactions explicites d'ordre √©lev√©. Configuration : [256, 128].

DNN : G√©n√©ralisation via un r√©seau profond. Configuration : [512 -> 256 -> 1].

‚öôÔ∏è Configuration Optimis√©e

Le fichier xdeepfm_config.yaml utilise les hyperparam√®tres suivants pour la performance :

Param√®tre

Valeur

Description

Embedding Dim

128

Haute r√©solution, crucial pour le CIN.

CIN Layers

[256, 128]

Capture des interactions d'ordre 2 et 3.

Batch Size

4096

Stabilise l'apprentissage.

Optimiseur

AdamW

Meilleure gestion du Weight Decay (1e-5).

Dropout

0.25

Pr√©vient le sur-apprentissage dans le DNN.

üöÄ Instructions d'Entra√Ænement

1. Installation

pip install torch pandas numpy pyarrow pyyaml tqdm scikit-learn


2. Lancer l'entra√Ænement

python src/train_xdeepfm.py


Le mod√®le sera sauvegard√© dans checkpoints/xDeepFM_best.pth.

3. G√©n√©rer la Soumission

python src/inference_xdeepfm.py


Cela g√©n√©rera le fichier submission_xdeepfm.zip pr√™t pour le leaderboard.

D√©velopp√© pour la comp√©tition MMCTR 2025.
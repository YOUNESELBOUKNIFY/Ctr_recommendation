# ========================
# Base configuration
# ========================
base_config:
  model_root: './checkpoints/'
  logs_root: './logs/'
  num_workers: 4
  verbose: 1
  save_best_only: True
  seed: 2025

# ========================
# Experiment Settings
# ========================
base_expid: MM_BST_Run
dataset_id: MicroLens_1M_x1

# ========================
# Dataset configuration
# ========================
dataset_config:
  MicroLens_1M_x1:
    data_root: ../data/MicroLens_1M_x1/
    data_format: parquet
    train_data: ../data/MicroLens_1M_x1/train.parquet
    valid_data: ../data/MicroLens_1M_x1/valid.parquet
    test_data: ../data/MicroLens_1M_x1/test.parquet
    item_info: ../data/MicroLens_1M_x1/item_info.parquet
    feature_cols:
      - {name: user_id, active: True, dtype: int, type: meta}
      - {name: item_seq, active: True, dtype: int, type: meta} # CRITIQUE pour BST
      - {name: likes_level, active: True, dtype: int, type: categorical, vocab_size: 11}
      - {name: views_level, active: True, dtype: int, type: categorical, vocab_size: 11}
      - {name: item_id, active: True, dtype: int, type: categorical, vocab_size: 91718, source: item}
      - {name: item_emb_d128, active: True, dtype: float, type: embedding, source: item, embedding_dim: 128}
    label_col: {name: label, dtype: float}

# ========================
# Model Configuration (BST + DCN)
# ========================
MM_BST_Run:
  model: MM_BST
  dataset_id: MicroLens_1M_x1
  loss: binary_crossentropy
  metrics: [AUC, logloss]

  # ---- Hyperparamètres Principaux ----
  learning_rate: 0.0005  # Les Transformers aiment les LR plus bas que DIN
  batch_size: 4096       # Ajuster selon VRAM (2048 si OOM)
  embedding_dim: 128     # Doit être divisible par transformer_n_heads
  max_len: 20            # Longueur de l'historique (BST gère bien les longues séquences)
  
  # ---- Transformer Config (BST) ----
  transformer_n_heads: 4
  transformer_n_layers: 1 # 1 couche suffit souvent, max 2
  transformer_dropout: 0.1
  
  # ---- Deep Cross Network Config ----
  dcn_layers: 2
  
  # ---- MLP Final ----
  dnn_hidden_units: [256, 128, 64]
  net_dropout: 0.2
  
  # ---- Entraînement ----
  epochs: 300
  optimizer: adamw       # AdamW est souvent meilleur pour Transformer
  weight_decay: 1e-4
  shuffle: True
  monitor: "AUC"
  monitor_mode: "max"
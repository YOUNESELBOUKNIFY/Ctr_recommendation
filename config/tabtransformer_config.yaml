# ========================
# Base experiment
# ========================
base_expid: TabTransformer_default
dataset_id: MicroLens_1M_x1

# ========================
# Dataset configuration
# ========================
dataset_config:
  MicroLens_1M_x1:
    data_root: ./data/MicroLens_1M_x1/
    data_format: parquet
    train_data: ./data/MicroLens_1M_x1/train.parquet
    valid_data: ./data/MicroLens_1M_x1/valid.parquet
    test_data: ./data/MicroLens_1M_x1/test.parquet
    item_info: ./data/MicroLens_1M_x1/item_info.parquet
    rebuild_dataset: False
    feature_cols:
      - {name: user_id, active: True, dtype: int, type: meta}
      - {name: item_seq, active: True, dtype: int, type: meta}
      - {name: likes_level, active: True, dtype: int, type: categorical, vocab_size: 11}
      - {name: views_level, active: True, dtype: int, type: categorical, vocab_size: 11}
      - {name: item_id, active: True, dtype: int, type: categorical, vocab_size: 91718, source: item}
      - {name: item_tags, active: True, dtype: int, type: sequence, max_len: 5, vocab_size: 11740, source: item}
      - {name: item_emb_d128, active: True, dtype: float, type: embedding, source: item, embedding_dim: 128}
    label_col: {name: label, dtype: float}

# ========================
# Model configuration
# ========================
TabTransformer_default:
  seed: 20242025
  model: TabTransformer
  dataset_id: MicroLens_1M_x1
  task: binary_classification
  loss: binary_crossentropy
  metrics: [AUC, logloss]

  # ---- TRAINING ----
  optimizer: adam
  learning_rate: 1e-3
  batch_size: 8192
  epochs: 100
  shuffle: True
  accumulation_steps: 1
  monitor: "AUC"
  monitor_mode: "max"

  # ---- REGULARIZATION ----
  embedding_regularizer: 1e-6
  net_regularizer: 0
  net_dropout: 0.1
  batch_norm: True

  # ---- MODEL HYPERPARAMS ----
  embedding_dim: 64
  transformer_n_heads: 4
  transformer_n_layers: 2
  transformer_dropout: 0.1
  dnn_hidden_units: [512, 256, 128]
  dnn_activations: ReLU

  # ---- DATA ----
  max_len: 5

# ========================
# Tuner space (optionnel pour hyperparam tuning)
# ========================
tuner_space:
  embedding_regularizer: [1e-6, 1e-7]
  net_regularizer: [0]
  net_dropout: [0.1]
  learning_rate: [1e-3]
  batch_size: [8192]
